Introduction
If speech is broken down into its fundamental building blocks, it is generally accepted that the smallest unit of an utterance is a speech sound, such as the vowel /æ/ found in the word “bat” or the consonant /ð/ found in “the”; these form the segmental aspect of speech. The way these sounds are combined to form meaningful units such as words, or sentences, is what is referred to as the suprasegmental aspect of speech, or more compactly: prosody. It is the interaction of four quantities that are generally associated with the melody and rhythm of speech, namely:
Fundamental frequency (f0) of the voice, or pitch; also known as intonation when referring to prosody (e.g., rising intonation or pitch contour when asking a question)
Syllable/vowel duration
Amplitude or intensity of the voice (e.g., speaking loudly vs. speaking calmly)
Phrasal and lexical stress (e.g., lexical: record [verb] vs. record [noun])
This paper is interested in how prosody is perceived across languages, and how it is affected by the acquisition of a new language. Firstly, it looks at the importance of prosody in the cross-linguistic study of different languages, then, it tackles the perception of foreign languages via the prosody framework, and finally, how it changes when a new language is acquired.
The Prosody Paradigm
Prosody is the way in which speech data is packaged and compressed. It is done with extreme efficiency, combining syntax, lexicon, pragmatics, and speaker affect by modulating the four suprasegmental parameters. This system is so complicated, that until this day, scientists are having trouble trying to decipher its complexities, in their efforts to have them reproduced in computer systems such as Siri or Cortana – to name the most common ones. It is therefore worthwhile to introduce this notable feature of speech, and outline its most prominent functions, and to what ends they are used.
Prosody Starts Early On
According to some studies like Mampe, Friederici, Christophe, & Wermke (2009), prosody acquisition starts at the prenatal stage. Since prosodic information lies in the low frequency spectrum of the voice, and since the maternal abdominal barrier acts as a good high frequency insulator, then segmental cues die out going through the uterus, and fetuses only receive the lowest frequencies from the outside world. In other words, they only “hear” prosody. Despite not being the first to do so, Mampe et al. found that newborn babies’ cries mimicked the pitch contours of their native language. While this may not be a ground-breaking discovery, it serves as further proof that prosody is an important aspect of language which we acquire before any other language feature. This warrants the investment of a greater amount of effort in this field of research, which has had a record of being overlooked in the past (Johnstone, 2001). Similarly, infants as young as 9 months not only replicate their native tongue’s prosody in their cries, but according to some studies (Esteve-Gibert & Prieto, 2013), seem to be able to communicate some distinct pragmatic intensions by modulating their pitch range and the duration of their babbling. Therefore, prosody is one of our first means of communication.
Different Kinds of Prosody
Prosody melds itself in many aspects of human expression. On the syntactic side, prosody creates a system of acoustical punctuation, without which, speech would sound like an endless string of meaningless sounds. Through this syntactic prosody model, a sort of vocal grammar is created as words are grouped into meaningful syntactic sets. Another perspective is emphatic prosody, which builds on syntactic prosody to make listeners acknowledge what the speaker is really trying to say or infer. It serves as a tool for interpretation, and is essential in giving meaning to words beyond their syntax. Therefore, when syntactic prosody fails, emphatic prosody can be used to disambiguate meaning. Another instance of emphatic prosody arises when we are internally reading a passage of text. Say for example that some reader is aware that the character in a book is Indian. Then while reading, they will change their internal prosody  to accommodate for that knowledge (Erekson, 2010). Hence, prosody is at the heart of many linguistic problem-solving tactics. 
Prosody in Processing Emotions
Prosody also serves as a regulator of conversations, from signaling turn taking cues to expressing emotions. Nevertheless, when it comes to comparing vocal expression to facial expression, it turns out that advances in the documentation of facial expressions are considerably ahead, considering that scientists can accurately determine an emotion based on a subject’s facial expression (Johnstone, 2001). Hence, no definite links have been found between specific emotions and acoustic cues. This might be due to the fact that humans rely more on visual data rather than their auditory sense (Politzer, 2008), or that technical considerations relating to audio signal processing have not allowed for much development. One only needs to look at the most trivial example of emotional projection that is at the heart of our everyday lives: emphatic writing. This happens a lot during text message conversations, in forms ranging from simple emojis, to repeating letters at the word level to express prosody. Failure to use these compensation strategies can lead to misunderstandings between two interlocuters, and this is all due to a lack of prosodic input.
Effect of Prosody on Foreign Language Perception
With the rise of globalization, and the cultural merge which the world is seeing, it is more and more important to control the ways through which second languages are learned, on the road to bilingualism. Putting more weight on including suprasegmentals as part of the learning experience could be a solution to solving many comprehensibility problems (Crowther, Trofimovich, Saito, & Isaacs, 2015). This part deals with how foreign languages are perceived by native speakers from a prosodic perspective, and how this might affect comprehensibility.
Prosody in Foreign Accent Authentication
It is interesting to look at what makes a specific language sound the way it does. Foreign languages are often recognizable, but what makes them so is still unclear. Ulbrich & Mennen (2016), in their introduction, are quite vocal about the ongoing debate between what has a greater effect on foreign accent (FA) perception: segmental or suprasegmental features. In other words, is prosody or the quality of a speech sound more indicative of how a foreign accent sounds? Two main currents dominate this specific topic, as different studies are split between segments being dominant over prosody, while others state the opposite. These studies are based on experiments which consist in the separation of prosody and segments in a speech fragment, or the splicing (transplantation) of prosody from a specific language into the segments of another (e.g., speech fragment with German segments and French prosody), and observing the qualitative responses of participants, on the authenticity of what they heard. They concluded their study by stating that there exists an intricate relationship between both segments and prosody, which makes for an interdependence of both when it comes to FA perception.
Differences in Turn-taking across Languages from a Prosodic Perspective
In a study conducted by Ward & Bayyari (2010), differences between American English (AE) and Egyptian Arabic turn-taking cues are outlined. During conversation, speakers tend to produce acoustic cues which call on the listener to respond with a back-channeling cue (e.g., “uh-huh”, “yeah” in AE) to confirm that they are still listening and are in sync with the speaker. Analysis of Egyptian Arabic speech has shown that one of these acoustic cues produced by speakers is something called a pitch downslope, which is a rapid descending incidence of the pitch contour. One of their experiments consisted in asking native American speakers to qualitatively describe how positive or negative this downslope sounded in a prerecorded conversation which involved back-channeling. The results fit their hypothesis which stated that American subjects would perceive the downslope as negative, and that this would cause slight misunderstandings between American and Arabic speakers. According to them, any considerable amount of exposure to Arabic prosody would be sufficient to clear those misunderstandings. But objectively speaking, from a holistic approach, this phenomenon is bound to affect the way non-Arabic speakers perceive, and understand speakers with Arabic L1 due to prosody transfer which is covered in a later section.
Prosody Transfer from First to Second Language
In Lebanese Arabic, /btaʕrfe/ (when addressing a female) or /btaʕrɪf/ (for a man) literally translates into “You know,” and can be used at the beginning of a sentence with a rising pitch contour, to prepare the listener for something the speaker is about to say. In AE, “you know” is mainly used as a gap filler, at the end of a phrase to solicit consent from the listener (Bortfeld, Leon, Ì, Bloom, & Schober, 2001). So one could imagine a scenario where an English-speaking Lebanese bilingual would say /btaʕrfe(ɪf)/ at the beginning of a phrase with a rising contour, thinking that this is the proper way of leading into a sentence, while in fact, it would just sound like a misplaced gap filler at the beginning of a sentence to an AE speaker. This is what is known as prosodic transfer from first language (L1) to second language (L2) in bilinguals, and specifically, intonation in this example. It is worthy to note that research on acquiring intonation is relatively new (Kainada & Lengeris, 2015).
Examples from Different Languages and Consequences on L2 acquisition
Prosody can be divided into two overarching ensembles: intonation and rhythm. When describing the rhythm of a language, there is a dichotomy between what we call stress-timed and syllable-timed languages. The most prominent languages that fall under the stress-timed category are English, Arabic, and Russian, whereas Chinese (Mandarin), French, Spanish and Italian are examples of syllable-timed languages. In stress-timed languages, stresses are distributed at regularly spaced intervals in time (e.g., ca. 0.6 seconds in AE), while in syllable-timed languages, there are no stressed syllables, and they generally have the same duration, with the exception of some emphasis and lengthening effects such as the syllable-final lengthening effect which is common in French (House, 2009). For this reason, syllable timed-languages are often described as having a “machine-gun” rhythm. Distributing stresses evenly in time, in the case of stress-timed languages, has the effect of distributing unstressed syllables’ durations very unevenly in-between stresses to maintain a regular pulse dictated by stress. On the other hand, there is no strict dichotomy for intonation, which can take on multiple facets, depending on the situation (i.e., asking a question, making an assertion, etc.). Now that these distinctions are clearer, a comparison can be drawn between English and other languages to underline the different prosodic transfers that can take place from L1 to L2.
Comparison: Chinese (Mandarin) and AE Rhythm
Chinese learners of English often run into difficulties in producing speech that mimics a native-like accent of English. First off all, this is due to the fact that Chinese is a syllable-stressed language and does not have stresses such as English. Therefore, Chinese learners of English tend to assign random stresses in words when speaking English, because all Chinese syllables generally receive equal stress (Zhang & Yin, 2009). This results in a decrease in speech intelligibility, due to the importance of stress in lexical determination, and overall flow of speech (Field, 2005). Another difference between Mandarin and AE rhythm is the lack of juncture between syllables in Chinese. According to Zhang & Yin (2009), if the AE flow resembles that of a violin playing legato, then Chinese would be comparable to a piano played in staccato style. This feature of Chinese is often seen in Chinese L2 learners of English as they fail to produce liaisons between syllables in their speech.
Comparison: Chinese (Mandarin) and AE Intonation
Chinese intonation is significantly different than AE in that Mandarin Chinese has tones (changes in pitch) which can alter lexical meaning. An “Algebraic Sum” theory was posited, which states that tones are integrated into intonation like small ripples riding on large waves in Chinese tonal dialects. This means that in addition to lexical tones, Chinese also exhibit pitch changes as part of their emphatic intonation patterns (Pan, 2012). Other differences also cause a transfer of prosodic features from Chinese to English. For instance, the unit of intonation in Chinese is the sentence, while in English it is smaller. This explains why English speakers seem to exhibit more fluctuation in pitch than Chinese learners of English, which retain their mother tongue’s intonation unit, even in English. Similarly, in Mandarin, intonation patterns often lie in the tail (end) of the intonational unit, therefore, as they speak in English, they tend to put more weight on the tail of a sentence when coming up with intonation patterns (Pan, 2012).
Therefore, with the support of this data, one can subjectively make an assumption about the difficulties prosody poses on learners of English coming from diverse L1 backgrounds, and how this background can either make it easier or harder for them to tune into the second language (Crowther et al., 2015). Learners should study prosody in addition to learning correct pronunciation of vowels and consonants to make sentence structure clearer in their perception of the L2, which is greatly influenced by L1 patterns.
Comparison: Greek and AE Intonation
Another example, is a study done by Kainada & Lengeris (2015) in which prosodic transfers from Greek to AE were analyzed. According to them, not until recently, has L1 background been taken into account in conducting prosodic transfer experiments, and that cross-linguistic similarities were not fully taken into account in designing experiments. In one of their hypotheses, they stated that by virtue of Greek and AE having significantly different intonation patterns in asking polar questions, Greek learners of AE would have no problem acquiring the AE intonation for these questions, on the premise that dissimilar sounds are easier to acquire. Results contradict the hypothesis, as full L1 prosodic transfer happened concerning polar questions asked in English by native Greeks. However, it was shown as part of another one of their hypotheses that speech rate was slower, pitch span was narrower, and pitch level was lower in the Greeks’ L2 (English), than in their L1. Possible explanations would include hesitation, difficulty in production, and uncertainty. But most importantly, the main conclusion was that some aspects of prosody are transferred (i.e., intonation), while others are not, resulting in an interlanguage form.
Prosody Transfer in Composers Through Music as an L2
In a study by Patel, Iversen, & Rosenberg (2006), the normalized pairwise variability index (nPVI) – a measure of durational contrast between successive elements in a sequence – and coefficient variation (CV) – used to make sure that durational contrast is not due to variability differences between the two languages – are monitored in the music of English and French composers. Applying those indices to both the rhythm and melody (through a prosogram) of both languages, and then to samples of music by the different composers, it was found by a quantitative comparison that composers speaking different languages statistically reflect this difference in their music. Therefore, it seems that prosody transfer can not only occur between languages, but also between language and music. Furthermore, this finding also corroborates the hypotheses that music and language are analogous in construct and share some common neural pathways (Jackendoff, 2009).
